{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abe67b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1f4952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "try:\n",
    "    # Load all datasets except for the press releases\n",
    "    advisor_df = pd.read_csv('data/advisor_data_labeled.csv', parse_dates=['date'])\n",
    "    financial_df = pd.read_csv('data/financial_data_labeled.csv', parse_dates=['date'])\n",
    "    market_df = pd.read_csv('data/market_data_labeled.csv', parse_dates=['date'])\n",
    "    social_df = pd.read_csv('data/raw_social_data_labeled (1).csv', parse_dates=['date'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please make sure all required CSV files are in the same directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2ff157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating social media data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Pre-processing and Feature Engineering ---\n",
    "print(\"Aggregating social media data...\")\n",
    "# Aggregate raw social data to get daily features per company\n",
    "social_agg = social_df.groupby([social_df['date'].dt.date, 'company']).agg(\n",
    "    social_post_count=('post_text', 'count'),\n",
    "    avg_likes=('likes', 'mean'),\n",
    "    avg_retweets=('retweets', 'mean'),\n",
    "    label=('label', 'max') # Take the max label for the day as the fraud indicator\n",
    ").reset_index()\n",
    "social_agg['date'] = pd.to_datetime(social_agg['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d112c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3395 entries, 0 to 3394\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   date               3395 non-null   datetime64[ns]\n",
      " 1   company            3395 non-null   object        \n",
      " 2   social_post_count  3395 non-null   int64         \n",
      " 3   avg_likes          3395 non-null   float64       \n",
      " 4   avg_retweets       3395 non-null   float64       \n",
      " 5   label              3395 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 159.3+ KB\n"
     ]
    }
   ],
   "source": [
    "social_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f7c1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Merge Data ---\n",
    "print(\"Merging data...\")\n",
    "# Start with market_df as the base\n",
    "merged_df = market_df.copy()\n",
    "\n",
    "# Merge financial data on company name\n",
    "merged_df = pd.merge(merged_df, financial_df.drop(columns=['date'], errors='ignore'), on='company', how='left', suffixes=('', '_fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee56daea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "Tata Motors    343805\n",
       "Reliance       318864\n",
       "Infosys        287226\n",
       "Wipro          286752\n",
       "TCS               544\n",
       "SBI               518\n",
       "ICICI             516\n",
       "Adani             491\n",
       "ONGC              487\n",
       "HDFC              466\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd82674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating company-specific anomaly features...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Company-Specific Anomaly Features ---\n",
    "print(\"Creating company-specific anomaly features...\")\n",
    "merged_df = merged_df.sort_values(by=['company', 'date']).reset_index(drop=True)\n",
    "\n",
    "features_to_engineer = ['price_change_1d', 'volume_spike', 'abnormal_return']\n",
    "window = 30 # 30-day rolling window\n",
    "\n",
    "for feature in features_to_engineer:\n",
    "    if feature in merged_df.columns:\n",
    "        rolling_stats = merged_df.groupby('company')[feature].rolling(window=window, min_periods=5)\n",
    "        rolling_mean = rolling_stats.mean().reset_index(level=0, drop=True)\n",
    "        rolling_std = rolling_stats.std().reset_index(level=0, drop=True)\n",
    "        merged_df[f'{feature}_zscore'] = (merged_df[feature] - rolling_mean) / (rolling_std + 1e-6)\n",
    "\n",
    "zscore_cols = [f for f in merged_df.columns if '_zscore' in f]\n",
    "# merged_df[zscore_cols] = merged_df[zscore_cols].fillna(np.median(merged_df[merged_df[zscore_cols].notna().sum(axis=1) > 0][zscore_cols]))\n",
    "merged_df[zscore_cols] = merged_df[zscore_cols].fillna(merged_df[zscore_cols].median())\n",
    "# merged_df = merged_df.dropna(subset=['revenue_growth']) # Drop rows where financial data is missing\n",
    "merged_df['revenue_growth'] = merged_df['revenue_growth'].fillna(merged_df['revenue_growth'].median())\n",
    "merged_df['profit_margin'] = merged_df['profit_margin'].fillna(merged_df['profit_margin'].median())\n",
    "merged_df['debt_to_equity'] = merged_df['debt_to_equity'].fillna(merged_df['debt_to_equity'].median())\n",
    "merged_df['roe'] = merged_df['roe'].fillna(merged_df['roe'].median())\n",
    "merged_df['insider_trades'] = merged_df['insider_trades'].fillna(merged_df['insider_trades'].median())\n",
    "merged_df['label_fin'] = merged_df['label_fin'].fillna(0)\n",
    "merged_df['filing_type'] = merged_df['filing_type'].fillna('Others')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0201ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1239669 entries, 0 to 1239668\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count    Dtype         \n",
      "---  ------                  --------------    -----         \n",
      " 0   date                    1239669 non-null  datetime64[ns]\n",
      " 1   company                 1239669 non-null  object        \n",
      " 2   price_change_1d         1239669 non-null  float64       \n",
      " 3   price_change_5d         1239669 non-null  float64       \n",
      " 4   volatility_post         1239669 non-null  float64       \n",
      " 5   abnormal_return         1239669 non-null  float64       \n",
      " 6   volume_spike            1239669 non-null  float64       \n",
      " 7   label                   1239669 non-null  int64         \n",
      " 8   revenue_growth          1239669 non-null  float64       \n",
      " 9   profit_margin           1236647 non-null  float64       \n",
      " 10  debt_to_equity          1236647 non-null  float64       \n",
      " 11  roe                     1236647 non-null  float64       \n",
      " 12  insider_trades          1236647 non-null  float64       \n",
      " 13  filing_type             1236647 non-null  object        \n",
      " 14  label_fin               1236647 non-null  float64       \n",
      " 15  price_change_1d_zscore  1239669 non-null  float64       \n",
      " 16  volume_spike_zscore     1239669 non-null  float64       \n",
      " 17  abnormal_return_zscore  1239669 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(14), int64(1), object(2)\n",
      "memory usage: 170.2+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38ead7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                      0\n",
       "company                   0\n",
       "price_change_1d           0\n",
       "price_change_5d           0\n",
       "volatility_post           0\n",
       "abnormal_return           0\n",
       "volume_spike              0\n",
       "label                     0\n",
       "revenue_growth            0\n",
       "profit_margin             0\n",
       "debt_to_equity            0\n",
       "roe                       0\n",
       "insider_trades            0\n",
       "filing_type               0\n",
       "label_fin                 0\n",
       "price_change_1d_zscore    0\n",
       "volume_spike_zscore       0\n",
       "abnormal_return_zscore    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab266c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "Tata Motors    343805\n",
       "Reliance       318864\n",
       "Infosys        287226\n",
       "Wipro          286752\n",
       "TCS               544\n",
       "SBI               518\n",
       "ICICI             516\n",
       "Adani             491\n",
       "ONGC              487\n",
       "HDFC              466\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5cf5132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing data for training...\n",
      "Saved company-to-category mapping to 'company_to_category_map.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Final Data Preparation ---\n",
    "print(\"Finalizing data for training...\")\n",
    "\n",
    "# --- ADD THIS PART ---\n",
    "# Convert company name to a numerical category\n",
    "merged_df['company_cat'] = merged_df['company'].astype('category').cat.codes\n",
    "\n",
    "# Create and save a mapping file for later use in prediction\n",
    "company_mapping = merged_df[['company', 'company_cat']].drop_duplicates().reset_index(drop=True)\n",
    "company_mapping.to_csv('data/company_to_category_map.csv', index=False)\n",
    "print(\"Saved company-to-category mapping to 'company_to_category_map.csv'\")\n",
    "# --- END OF ADDITION ---\n",
    "\n",
    "label_cols = [col for col in merged_df.columns if 'label' in col]\n",
    "y = merged_df[label_cols].max(axis=1).fillna(0).astype(int)\n",
    "\n",
    "features_to_drop = features_to_engineer + ['date', 'company', 'filing_type'] + label_cols\n",
    "X = merged_df.drop(columns=features_to_drop)\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16ae3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data and training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 480699, number of negative: 449052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 929751, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.517019 -> initscore=0.068103\n",
      "[LightGBM] [Info] Start training from score 0.068103\n",
      "\n",
      "--- Model 1 Evaluation ---\n",
      "Accuracy: 0.9001\n",
      "AUC Score: 0.9812\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90    149684\n",
      "           1       0.98      0.82      0.90    160234\n",
      "\n",
      "    accuracy                           0.90    309918\n",
      "   macro avg       0.91      0.90      0.90    309918\n",
      "weighted avg       0.91      0.90      0.90    309918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Model Training and Evaluation ---\n",
    "print(\"Splitting data and training LightGBM model...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n--- Model 1 Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73ba26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and feature list...\n",
      "\n",
      "Model 1 (market-financial) and its feature list have been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Save Artifacts ---\n",
    "print(\"Saving model and feature list...\")\n",
    "joblib.dump(lgb_model, 'model_data/model_1_market_financial.joblib')\n",
    "joblib.dump(list(X.columns), 'model_data/model_1_market_financial_features.joblib')\n",
    "X_test.to_csv('test_data/model_1_X_test_data.csv', index=False)\n",
    "y_test.to_csv('test_data/model_1_y_test_data.csv', index=False)\n",
    "print(\"\\nModel 1 (market-financial) and its feature list have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

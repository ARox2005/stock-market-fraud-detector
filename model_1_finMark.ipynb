{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe67b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1f4952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "try:\n",
    "    # Load all datasets except for the press releases\n",
    "    advisor_df = pd.read_csv('data/advisor_data_labeled.csv', parse_dates=['date'])\n",
    "    financial_df = pd.read_csv('data/financial_data_labeled.csv', parse_dates=['date'])\n",
    "    market_df = pd.read_csv('data/market_data_labeled.csv', parse_dates=['date'])\n",
    "    social_df = pd.read_csv('data/raw_social_data_labeled (1).csv', parse_dates=['date'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please make sure all required CSV files are in the same directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2ff157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating social media data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Pre-processing and Feature Engineering ---\n",
    "print(\"Aggregating social media data...\")\n",
    "# Aggregate raw social data to get daily features per company\n",
    "social_agg = social_df.groupby([social_df['date'].dt.date, 'company']).agg(\n",
    "    social_post_count=('post_text', 'count'),\n",
    "    avg_likes=('likes', 'mean'),\n",
    "    avg_retweets=('retweets', 'mean'),\n",
    "    label=('label', 'max') # Take the max label for the day as the fraud indicator\n",
    ").reset_index()\n",
    "social_agg['date'] = pd.to_datetime(social_agg['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d112c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3395 entries, 0 to 3394\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   date               3395 non-null   datetime64[ns]\n",
      " 1   company            3395 non-null   object        \n",
      " 2   social_post_count  3395 non-null   int64         \n",
      " 3   avg_likes          3395 non-null   float64       \n",
      " 4   avg_retweets       3395 non-null   float64       \n",
      " 5   label              3395 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 159.3+ KB\n"
     ]
    }
   ],
   "source": [
    "social_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f7c1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Merge Data ---\n",
    "print(\"Merging data...\")\n",
    "# Start with market_df as the base\n",
    "merged_df = market_df.copy()\n",
    "\n",
    "# Merge financial data on company name\n",
    "merged_df = pd.merge(merged_df, financial_df.drop(columns=['date'], errors='ignore'), on='company', how='left', suffixes=('', '_fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd82674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating company-specific anomaly features...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Company-Specific Anomaly Features ---\n",
    "print(\"Creating company-specific anomaly features...\")\n",
    "merged_df = merged_df.sort_values(by=['company', 'date']).reset_index(drop=True)\n",
    "\n",
    "features_to_engineer = ['price_change_1d', 'volume_spike', 'abnormal_return']\n",
    "window = 30 # 30-day rolling window\n",
    "\n",
    "for feature in features_to_engineer:\n",
    "    if feature in merged_df.columns:\n",
    "        rolling_stats = merged_df.groupby('company')[feature].rolling(window=window, min_periods=5)\n",
    "        rolling_mean = rolling_stats.mean().reset_index(level=0, drop=True)\n",
    "        rolling_std = rolling_stats.std().reset_index(level=0, drop=True)\n",
    "        merged_df[f'{feature}_zscore'] = (merged_df[feature] - rolling_mean) / (rolling_std + 1e-6)\n",
    "\n",
    "zscore_cols = [f for f in merged_df.columns if '_zscore' in f]\n",
    "merged_df[zscore_cols] = merged_df[zscore_cols].fillna(0)\n",
    "merged_df = merged_df.dropna(subset=['revenue_growth']) # Drop rows where financial data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5cf5132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing data for training...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Final Data Preparation ---\n",
    "print(\"Finalizing data for training...\")\n",
    "label_cols = [col for col in merged_df.columns if 'label' in col]\n",
    "y = merged_df[label_cols].max(axis=1).fillna(0).astype(int)\n",
    "\n",
    "features_to_drop = features_to_engineer + ['date', 'company', 'filing_type'] + label_cols\n",
    "X = merged_df.drop(columns=features_to_drop)\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16ae3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data and training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 479773, number of negative: 447712\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2308\n",
      "[LightGBM] [Info] Number of data points in the train set: 927485, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.517284 -> initscore=0.069163\n",
      "[LightGBM] [Info] Start training from score 0.069163\n",
      "\n",
      "--- Model 1 Evaluation ---\n",
      "Accuracy: 0.8845\n",
      "AUC Score: 0.9740\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89    149237\n",
      "           1       0.97      0.80      0.88    159925\n",
      "\n",
      "    accuracy                           0.88    309162\n",
      "   macro avg       0.90      0.89      0.88    309162\n",
      "weighted avg       0.90      0.88      0.88    309162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Model Training and Evaluation ---\n",
    "print(\"Splitting data and training LightGBM model...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n--- Model 1 Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73ba26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and feature list...\n",
      "\n",
      "Model 1 (market-financial) and its feature list have been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Save Artifacts ---\n",
    "print(\"Saving model and feature list...\")\n",
    "joblib.dump(lgb_model, 'model_data/model_1_market_financial.joblib')\n",
    "joblib.dump(list(X.columns), 'model_data/model_1_market_financial_features.joblib')\n",
    "X_test.to_csv('test_data/model_1_X_test_data.csv', index=False)\n",
    "y_test.to_csv('test_data/model_1_y_test_data.csv', index=False)\n",
    "print(\"\\nModel 1 (market-financial) and its feature list have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
